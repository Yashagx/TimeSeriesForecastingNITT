{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0UuUhf8TRJJZUBBvSW0sF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dSQJ4Z7a0QjQ"},"outputs":[],"source":["import os\n","import datetime\n","import tweepy\n","import requests\n","import pandas as pd\n","import torch\n","import demoji\n","import re\n","import numpy as np\n","from transformers import RobertaTokenizer, RobertaModel\n","from openpyxl import load_workbook\n","from openpyxl.utils import get_column_letter\n","\n","# === API KEYS ===\n","TWITTER_BEARER = 'AAAAAAAAAAAAAAAAAAAAAI3L2QEAAAAAo3LAhCkOPv2ouGm8qKV%2FA9WYM04%3DUt0sPDNtAOPdc4B11jeBf42YyVych65f5q2pNcSZEu100qk1ke'\n","NEWS_API_KEY = '1b6125d0311a4351994f2cb04c2ff887'\n","WEATHER_API_KEY = 'bd5e378503939ddaee76f12ad7a97608'\n","\n","# === Load RoBERTa ===\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaModel.from_pretrained(\"roberta-base\")\n","\n","# === Clean Text ===\n","def clean_text(text):\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"@\\w+\", \"\", text)\n","    text = re.sub(r\"#\\w+\", \"\", text)\n","    text = demoji.replace(text, \"\")\n","    text = re.sub(r\"RT\\s\", \"\", text)\n","    return text.strip()\n","\n","# === Get Embedding ===\n","def get_roberta_embedding(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    return outputs.last_hidden_state[:, 0, :].squeeze().tolist()\n","\n","# === Fetch Tweets (English Only) ===\n","def fetch_tweets(query=\"COVID-19\", max_results=10):\n","    client = tweepy.Client(bearer_token=TWITTER_BEARER)\n","    try:\n","        response = client.search_recent_tweets(query=f\"{query} lang:en\", max_results=max_results)\n","        return [tweet.text for tweet in response.data] if response.data else []\n","    except Exception as e:\n","        print(\"‚ö†Ô∏è Tweet fetch failed:\", e)\n","        return []\n","\n","# === Fetch News (English Only) ===\n","def fetch_news(query=\"COVID\", country=\"us\", page_size=10):\n","    url = \"https://newsapi.org/v2/top-headlines\"\n","    params = {\"q\": query, \"apiKey\": NEWS_API_KEY, \"pageSize\": page_size, \"language\": \"en\", \"country\": country}\n","    try:\n","        response = requests.get(url, params=params)\n","        return [article[\"title\"] for article in response.json().get(\"articles\", [])]\n","    except Exception as e:\n","        print(\"‚ö†Ô∏è News fetch failed:\", e)\n","        return []\n","\n","# === Clinical Data from OWID CSV ===\n","def fetch_clinical_from_csv(csv_path=\"owid-covid-data.csv\", country=\"India\"):\n","    try:\n","        if not os.path.exists(csv_path):\n","            print(f\"‚ùå File '{csv_path}' not found.\")\n","            return {k: None for k in [\"confirmed\", \"deaths\", \"confirmed_per_million\", \"new_cases\", \"new_deaths\"]}\n","\n","        df = pd.read_csv(csv_path)\n","        df = df[df[\"location\"] == country]\n","\n","        if df.empty:\n","            print(f\"‚ùå No data for country: {country}\")\n","            return {k: None for k in [\"confirmed\", \"deaths\", \"confirmed_per_million\", \"new_cases\", \"new_deaths\"]}\n","\n","        df = df.dropna(subset=[\"total_cases\", \"total_deaths\"])\n","        latest = df.sort_values(\"date\").iloc[-1]\n","\n","        return {\n","            \"confirmed\": round(latest.get(\"total_cases\", 0), 2),\n","            \"deaths\": round(latest.get(\"total_deaths\", 0), 2),\n","            \"confirmed_per_million\": round(latest.get(\"total_cases_per_million\", 0), 2),\n","            \"new_cases\": round(latest.get(\"new_cases\", 0), 2),\n","            \"new_deaths\": round(latest.get(\"new_deaths\", 0), 2)\n","        }\n","\n","    except Exception as e:\n","        print(\"‚ö†Ô∏è Error fetching clinical data:\", e)\n","        return {k: None for k in [\"confirmed\", \"deaths\", \"confirmed_per_million\", \"new_cases\", \"new_deaths\"]}\n","\n","# === Climate Data ===\n","def fetch_climate(city=\"Chennai\", country=\"IN\"):\n","    try:\n","        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&appid={WEATHER_API_KEY}&units=metric\"\n","        data = requests.get(url).json()\n","        main = data.get(\"main\", {})\n","        return {\n","            \"temperature\": round(main.get(\"temp\", 0), 1),\n","            \"feels_like\": round(main.get(\"feels_like\", 0), 1),\n","            \"humidity\": main.get(\"humidity\"),\n","            \"pressure\": main.get(\"pressure\"),\n","            \"visibility\": data.get(\"visibility\", None),\n","            \"wind_speed\": data.get(\"wind\", {}).get(\"speed\", None)\n","        }\n","    except Exception as e:\n","        print(\"‚ö†Ô∏è Climate fetch failed:\", e)\n","        return {\"temperature\": None, \"humidity\": None, \"pressure\": None}\n","\n","# === Auto-fit Excel Columns ===\n","def auto_adjust_column_widths(path):\n","    wb = load_workbook(path)\n","    ws = wb.active\n","    for col in ws.columns:\n","        max_len = max((len(str(cell.value)) for cell in col if cell.value), default=0)\n","        ws.column_dimensions[get_column_letter(col[0].column)].width = min(max_len + 3, 80)\n","    wb.save(path)\n","\n","# === Main Pipeline ===\n","def run_daily_pipeline():\n","    print(\"üöÄ Running multimodal pipeline...\")\n","    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n","    folder = \"multimodal_daily_data\"\n","    os.makedirs(folder, exist_ok=True)\n","\n","    tweets = fetch_tweets()\n","    news = fetch_news()\n","    texts = [clean_text(t) for t in tweets + news]\n","\n","    clinical = fetch_clinical_from_csv()\n","    climate = fetch_climate()\n","\n","    records = []\n","    embeddings = []\n","\n","    for text in texts:\n","        emb = get_roberta_embedding(text)\n","        row = {\n","            \"date\": today,\n","            \"text\": text,\n","            **clinical,\n","            **climate\n","        }\n","        records.append(row)\n","        embeddings.append(emb)\n","\n","    # Save metadata\n","    df_meta = pd.DataFrame(records)\n","    meta_path = f\"{folder}/{today}_predictive_metadata.xlsx\"\n","    df_meta.to_excel(meta_path, index=False, engine='openpyxl')\n","    auto_adjust_column_widths(meta_path)\n","    print(f\"‚úÖ Metadata saved: {meta_path}\")\n","\n","    # Save embeddings\n","    npy_path = f\"{folder}/{today}_embeddings.npy\"\n","    np.save(npy_path, np.array(embeddings))\n","    print(f\"‚úÖ Embeddings (.npy) saved: {npy_path}\")\n","\n","    # Optional: Save embeddings as CSV\n","    emb_csv_path = f\"{folder}/{today}_embeddings.csv\"\n","    pd.DataFrame(embeddings).to_csv(emb_csv_path, index=False)\n","    print(f\"‚úÖ Embeddings (.csv) saved: {emb_csv_path}\")\n","\n","# === Run Now ===\n","if __name__ == \"__main__\":\n","    run_daily_pipeline()"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","# === Paths ===\n","data_dir = \"multimodal_daily_data\"\n","days = sorted([f[:10] for f in os.listdir(data_dir) if f.endswith(\"_predictive_metadata.xlsx\")])\n","\n","X, Y, dates_used = [], [], []\n","\n","for i in range(len(days) - 1):\n","    today = days[i]\n","    tomorrow = days[i + 1]\n","\n","    try:\n","        meta_path = os.path.join(data_dir, f\"{today}_predictive_metadata.xlsx\")\n","        emb_path = os.path.join(data_dir, f\"{today}_embeddings.npy\")\n","        next_day_path = os.path.join(data_dir, f\"{tomorrow}_predictive_metadata.xlsx\")\n","\n","        meta_df = pd.read_excel(meta_path)\n","        embeddings = np.load(emb_path)\n","        target_df = pd.read_excel(next_day_path)\n","\n","        target_value = target_df[\"new_cases\"].mean()\n","\n","        for idx, row in meta_df.iterrows():\n","            emb = embeddings[idx] if idx < len(embeddings) else [0]*768\n","            features = list(emb) + [\n","                row.get(\"temperature\", 0),\n","                row.get(\"humidity\", 0),\n","                row.get(\"wind_speed\", 0),\n","                len(str(row[\"text\"]))\n","            ]\n","            X.append(features)\n","            Y.append(target_value)\n","            dates_used.append(today)\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Skipped {today}: {e}\")\n","\n","np.save(\"X_train.npy\", np.array(X))\n","np.save(\"Y_train.npy\", np.array(Y))\n","pd.DataFrame({\"date\": dates_used, \"target_new_cases\": Y}).to_csv(\"training_dates_summary.csv\", index=False)\n","\n","print(f\"‚úÖ Supervised dataset ready. X: {len(X)} samples, Y: {len(Y)} targets\")\n"],"metadata":{"id":"ZELoOMHA0WVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestRegressor\n","from datetime import datetime\n","import os\n","\n","# === Load training dataset ===\n","X_train = np.load(\"X_train.npy\")\n","Y_train = np.load(\"Y_train.npy\")\n","\n","# === Train model ===\n","model = RandomForestRegressor(n_estimators=100, random_state=42)\n","model.fit(X_train, Y_train)\n","\n","# === Load today's data ===\n","today_str = datetime.today().strftime(\"%Y-%m-%d\")\n","meta_path = f\"multimodal_daily_data/{today_str}_predictive_metadata.xlsx\"\n","emb_path = f\"multimodal_daily_data/{today_str}_embeddings.npy\"\n","\n","try:\n","    meta_df = pd.read_excel(meta_path)\n","    embeddings = np.load(emb_path)\n","except Exception as e:\n","    print(\"‚ùå Error loading today's data:\", e)\n","    exit()\n","\n","# === Prepare today's features ===\n","X_today = []\n","for idx, row in meta_df.iterrows():\n","    emb = embeddings[idx] if idx < len(embeddings) else [0]*768\n","    features = list(emb) + [\n","        row.get(\"temperature\", 0),\n","        row.get(\"humidity\", 0),\n","        row.get(\"wind_speed\", 0),\n","        len(str(row[\"text\"]))\n","    ]\n","    X_today.append(features)\n","\n","X_today = np.array(X_today)\n","preds = model.predict(X_today)\n","predicted_cases = preds.mean()\n","\n","print(f\"\\nüìà Predicted COVID-19 New Cases for {today_str}: {predicted_cases:.2f}\")\n","print(\"‚úÖ Prediction complete\")\n","\n","# === Optional: log prediction ===\n","with open(\"prediction_log.csv\", \"a\") as f:\n","    f.write(f\"{today_str},{predicted_cases:.2f}\\n\")\n","print(\"üì¶ Logged in prediction_log.csv\")\n"],"metadata":{"id":"t4qoNXMa0ZKL"},"execution_count":null,"outputs":[]}]}