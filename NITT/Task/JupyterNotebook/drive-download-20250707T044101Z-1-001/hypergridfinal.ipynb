{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXoW/NuqnG6E3er6tQXYZ8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bMMO8YaNAbrl"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Input\n","from tensorflow.keras.optimizers import Adam\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","# ==== Step 1: Load & Resample to Weekly ====\n","df = pd.read_csv(\"owid-covid-data.csv\", parse_dates=['date'])\n","df = df[df['location'] == 'India'][['date', 'total_cases', 'new_cases', 'total_deaths']]\n","df.dropna(inplace=True)\n","df.set_index('date', inplace=True)\n","\n","weekly_df = pd.DataFrame()\n","weekly_df['total_cases'] = df['total_cases'].resample('W').last()\n","weekly_df['new_cases'] = df['new_cases'].resample('W').sum()\n","weekly_df['total_deaths'] = df['total_deaths'].resample('W').last()\n","weekly_df.dropna(inplace=True)\n","\n","scaler = MinMaxScaler()\n","data_scaled = scaler.fit_transform(weekly_df)\n","\n","def create_dataset(dataset, look_back=4):\n","    X, Y = [], []\n","    for i in range(len(dataset) - look_back):\n","        X.append(dataset[i:i + look_back])\n","        Y.append(dataset[i + look_back])\n","    return np.array(X), np.array(Y)\n","\n","def objective(weights, y_true, preds):\n","    weights = np.array(weights)\n","    if np.sum(weights) == 0 or np.any(np.isnan(weights)):\n","        return 1e9\n","    weights = weights / np.sum(weights)\n","    ensemble = np.tensordot(weights, preds, axes=1)\n","    if np.any(np.isnan(ensemble)):\n","        return 1e9\n","    return mean_squared_error(y_true, ensemble)\n","\n","def scwoa_optimize(preds, y_true, dim=2, pop_size=20, max_iter=50):\n","    lb, ub = 0, 1\n","    population = np.random.uniform(lb, ub, (pop_size, dim))\n","    best_score, best_weights = float('inf'), None\n","\n","    for t in range(max_iter):\n","        a = 2 - t * (2 / max_iter)\n","        for i in range(pop_size):\n","            r1, r2 = np.random.rand(), np.random.rand()\n","            A = 2 * a * r1 - a\n","            C = 2 * r2\n","            if best_weights is None:\n","                Xp = population[i]\n","            else:\n","                p = np.random.rand()\n","                if p < 0.5:\n","                    D = abs(C * best_weights - population[i])\n","                    Xp = best_weights - A * D\n","                else:\n","                    l = np.random.uniform(-1, 1)\n","                    D = abs(best_weights - population[i])\n","                    Xp = D * np.exp(0.1 * l) * np.cos(2 * np.pi * l) + best_weights\n","            Xp = np.clip(Xp, lb, ub)\n","            fitness = objective(Xp, y_true, preds)\n","            if fitness < best_score:\n","                best_score = fitness\n","                best_weights = Xp\n","    return best_weights / np.sum(best_weights)\n","\n","# ==== Step 3: Create output folders ====\n","output_dir = \"prediction_outputs\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","summary_data = []\n","\n","# ==== Step 4: Evaluate for Each Target ====\n","target_indices = {'total_cases': 0, 'new_cases': 1, 'total_deaths': 2}\n","look_back = 4\n","epoch = 30\n","lr = 0.01\n","\n","for target_name, idx in target_indices.items():\n","    print(f\"\\n🔍 Target: {target_name}\")\n","\n","    X, y_all = create_dataset(data_scaled, look_back)\n","    y = y_all[:, idx]\n","\n","    train_size = int(len(X) * 0.7)\n","    X_train, y_train = X[:train_size], y[:train_size]\n","    X_test, y_test = X[train_size:], y[train_size:]\n","\n","    # ==== BPNN ====\n","    bpnn = MLPRegressor(hidden_layer_sizes=(64,), max_iter=500)\n","    bpnn.fit(X_train.reshape(len(X_train), -1), y_train)\n","    bpnn_pred = bpnn.predict(X_test.reshape(len(X_test), -1))\n","\n","    # ==== BPNN + Elman ====\n","    elman = Sequential([\n","        Input(shape=(look_back, 3)),\n","        SimpleRNN(32, activation='tanh'),\n","        Dense(1)\n","    ])\n","    elman.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n","    elman.fit(X_train, y_train, epochs=epoch, batch_size=8, verbose=0)\n","    elman_pred = elman.predict(X_test).flatten()\n","\n","    preds_elman = np.stack([bpnn_pred, elman_pred], axis=0)\n","    best_weights_elman = scwoa_optimize(preds_elman, y_test)\n","    final_pred_elman = np.tensordot(best_weights_elman, preds_elman, axes=1)\n","\n","    dummy = np.zeros((len(y_test), data_scaled.shape[1]))\n","    dummy[:, idx] = y_test\n","    actual_orig = scaler.inverse_transform(dummy)[:, idx]\n","    dummy[:, idx] = final_pred_elman\n","    pred_orig_elman = scaler.inverse_transform(dummy)[:, idx]\n","\n","    df_elman = pd.DataFrame({'Actual': actual_orig, 'Predicted': pred_orig_elman})\n","    path_elman = os.path.join(output_dir, f\"weekly_actual_vs_predicted_{target_name}_bpnn_elman.csv\")\n","    df_elman.to_csv(path_elman, index=False)\n","\n","    rmse_elman = np.sqrt(mean_squared_error(actual_orig, pred_orig_elman))\n","    mae_elman = mean_absolute_error(actual_orig, pred_orig_elman)\n","    summary_data.append({\n","        'Target': target_name,\n","        'Model': 'BPNN + Elman',\n","        'RMSE': rmse_elman,\n","        'MAE': mae_elman,\n","        'Weight_BPNN': best_weights_elman[0],\n","        'Weight_RNN': best_weights_elman[1]\n","    })\n","    print(f\"✅ BPNN + Elman → {path_elman}\")\n","\n","    # ==== BPNN + LSTM ====\n","    lstm = Sequential([\n","        Input(shape=(look_back, 3)),\n","        LSTM(32, activation='relu'),\n","        Dense(1)\n","    ])\n","    lstm.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n","    lstm.fit(X_train, y_train, epochs=epoch, batch_size=8, verbose=0)\n","    lstm_pred = lstm.predict(X_test).flatten()\n","\n","    preds_lstm = np.stack([bpnn_pred, lstm_pred], axis=0)\n","    best_weights_lstm = scwoa_optimize(preds_lstm, y_test)\n","    final_pred_lstm = np.tensordot(best_weights_lstm, preds_lstm, axes=1)\n","\n","    dummy[:, idx] = final_pred_lstm\n","    pred_orig_lstm = scaler.inverse_transform(dummy)[:, idx]\n","\n","    df_lstm = pd.DataFrame({'Actual': actual_orig, 'Predicted': pred_orig_lstm})\n","    path_lstm = os.path.join(output_dir, f\"weekly_actual_vs_predicted_{target_name}_bpnn_lstm.csv\")\n","    df_lstm.to_csv(path_lstm, index=False)\n","\n","    rmse_lstm = np.sqrt(mean_squared_error(actual_orig, pred_orig_lstm))\n","    mae_lstm = mean_absolute_error(actual_orig, pred_orig_lstm)\n","    summary_data.append({\n","        'Target': target_name,\n","        'Model': 'BPNN + LSTM',\n","        'RMSE': rmse_lstm,\n","        'MAE': mae_lstm,\n","        'Weight_BPNN': best_weights_lstm[0],\n","        'Weight_RNN': best_weights_lstm[1]\n","    })\n","    print(f\"✅ BPNN + LSTM → {path_lstm}\")\n","\n","# ==== Step 5: Save Summary ====\n","summary_df = pd.DataFrame(summary_data)\n","summary_path = os.path.join(output_dir, \"model_summary_results.csv\")\n","summary_df.to_csv(summary_path, index=False)\n","print(f\"\\n📊 Summary saved to: {summary_path}\")\n"]}]}