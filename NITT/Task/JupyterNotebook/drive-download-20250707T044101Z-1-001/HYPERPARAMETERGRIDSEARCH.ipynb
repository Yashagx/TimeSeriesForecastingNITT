{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPA4iXpYQJAkHkM1QnVuIWB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QEBTWag8oDX5"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Input\n","from tensorflow.keras.optimizers import Adam, Adamax\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","# Load and preprocess data\n","df = pd.read_csv(\"owid-covid-data.csv\")\n","df = df[df['location'] == 'India'][['total_cases', 'new_cases', 'total_deaths', 'new_deaths']]\n","df.dropna(inplace=True)\n","\n","scaler = MinMaxScaler()\n","data_scaled = scaler.fit_transform(df)\n","\n","def create_dataset(dataset, look_back=7):\n","    X, Y = [], []\n","    for i in range(len(dataset) - look_back):\n","        X.append(dataset[i:i+look_back])\n","        Y.append(dataset[i+look_back])\n","    return np.array(X), np.array(Y)\n","\n","look_back = 7\n","X, y = create_dataset(data_scaled, look_back)\n","train_size = int(len(X) * 0.7)\n","X_train, y_train = X[:train_size], y[:train_size]\n","X_test, y_test = X[train_size:], y[train_size:]\n","\n","# SCWOA optimization function\n","def objective(weights, y_true, preds):\n","    weights = np.array(weights)\n","    if np.sum(weights) == 0 or np.any(np.isnan(weights)):\n","        return 1e9\n","    weights = weights / np.sum(weights)\n","    ensemble = np.tensordot(weights, preds, axes=1)\n","    if np.any(np.isnan(ensemble)):\n","        return 1e9\n","    return mean_squared_error(y_true, ensemble)\n","\n","# Hyperparameter grid\n","epoch_list = [10, 30, 50, 100, 200]\n","lr_list = [0.2, 0.1, 0.05, 0.01, 0.001]\n","results = []\n","\n","for epochs in tqdm(epoch_list, desc=\"Epoch Loop\"):\n","    for lr in tqdm(lr_list, desc=f\"Learning Rate Loop for epoch={epochs}\", leave=False):\n","\n","        # Train BPNN\n","        bpnn = MLPRegressor(hidden_layer_sizes=(64,), max_iter=500)\n","        bpnn.fit(X_train.reshape(len(X_train), -1), y_train)\n","        bpnn_pred = bpnn.predict(X_test.reshape(len(X_test), -1))\n","\n","        # Train Elman RNN\n","        el_model = Sequential([\n","            Input(shape=(look_back, 4)),\n","            SimpleRNN(32, activation='tanh'),\n","            Dense(4)\n","        ])\n","        el_model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n","        el_model.fit(X_train, y_train, epochs=epochs, batch_size=16, verbose=0)\n","        elman_pred = el_model.predict(X_test)\n","\n","        # Train LSTM\n","        lstm_model = Sequential([\n","            Input(shape=(look_back, 4)),\n","            LSTM(32, activation='relu'),\n","            Dense(4)\n","        ])\n","        lstm_model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n","        lstm_model.fit(X_train, y_train, epochs=epochs, batch_size=16, verbose=0)\n","        lstm_pred = lstm_model.predict(X_test)\n","\n","        predictions = np.stack([bpnn_pred, elman_pred, lstm_pred], axis=0)\n","\n","        # SCWOA optimization\n","        pop_size, max_iter, dim = 20, 50, 3\n","        lb, ub = 0, 1\n","        population = np.random.uniform(lb, ub, (pop_size, dim))\n","        best_score, best_weights = float('inf'), None\n","\n","        for t in range(max_iter):\n","            a = 2 - t * (2 / max_iter)\n","            for i in range(pop_size):\n","                r1, r2 = np.random.rand(), np.random.rand()\n","                A = 2 * a * r1 - a\n","                C = 2 * r2\n","                if best_weights is None:\n","                    Xp = population[i]\n","                else:\n","                    p = np.random.rand()\n","                    if p < 0.5:\n","                        D = abs(C * best_weights - population[i])\n","                        Xp = best_weights - A * D\n","                    else:\n","                        l = np.random.uniform(-1, 1)\n","                        D = abs(best_weights - population[i])\n","                        Xp = D * np.exp(0.1 * l) * np.cos(2 * np.pi * l) + best_weights\n","                Xp = np.clip(Xp, lb, ub)\n","                fitness = objective(Xp, y_test, predictions)\n","                if fitness < best_score:\n","                    best_score = fitness\n","                    best_weights = Xp\n","\n","        # Final ensemble prediction\n","        best_weights = best_weights / np.sum(best_weights)\n","        final_pred = np.tensordot(best_weights, predictions, axes=1)\n","\n","        # Evaluate\n","        y_test_orig = scaler.inverse_transform(y_test)\n","        final_pred_orig = scaler.inverse_transform(final_pred)\n","        rmse = np.sqrt(mean_squared_error(y_test_orig, final_pred_orig))\n","        mae = mean_absolute_error(y_test_orig, final_pred_orig)\n","\n","        results.append({\n","            'Epochs': epochs,\n","            'Learning_Rate': lr,\n","            'RMSE': rmse,\n","            'MAE': mae,\n","            'BPNN_Weight': best_weights[0],\n","            'Elman_Weight': best_weights[1],\n","            'LSTM_Weight': best_weights[2]\n","        })\n","\n","# Results summary\n","results_df = pd.DataFrame(results)\n","print(\"\\n🔍 Top 5 Configurations by RMSE:\")\n","print(results_df.sort_values(by='RMSE').head())\n","\n","# Save results to CSV\n","results_df.to_csv(\"ensemble_gridsearch_results.csv\", index=False)\n"]}]}